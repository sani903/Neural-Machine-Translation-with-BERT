{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db8ba617",
   "metadata": {},
   "source": [
    "# Importing relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2a7aa57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "#__future__ to bring features from newer versions of Python \n",
    "# from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import string\n",
    "from string import digits\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import unicodedata\n",
    "\n",
    "import os\n",
    "\n",
    "from bert import BertModelLayer\n",
    "from bert.loader import StockBertConfig, load_stock_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7677532b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/jd/ll0by25n4ln09rznyqbztdyw0000gn/T/ipykernel_50656/3505786474.py:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "GPU Available: Metal device set to: Apple M1\n",
      " True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-30 04:22:51.568174: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-07-30 04:22:51.568617: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')\n",
    "print(\"GPU Available: \", tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660fd8cb",
   "metadata": {},
   "source": [
    "# Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91660001",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading data from NLD-ENG translation dataset\n",
    "with open('nld-eng/nld.txt','r') as f:\n",
    "  data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1de78cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68954"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing trailing and leading characters\n",
    "#creating list of word translations by splitting on new line\n",
    "uncleaned_data_list = data.strip().split('\\n')\n",
    "len(uncleaned_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4588a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi.\tHai!\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #6117420 (Raizin)\n"
     ]
    }
   ],
   "source": [
    "print(uncleaned_data_list[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7b62070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Go.', 'Lopen!', 'CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #7764436 (LinguisticFusion)']\n"
     ]
    }
   ],
   "source": [
    "for word in uncleaned_data_list:\n",
    "    print(word.split('\\t'))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c47d50e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Go.', 'Lopen!']\n"
     ]
    }
   ],
   "source": [
    "for word in uncleaned_data_list:\n",
    "    print(word.split('\\t')[:-1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97a0cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating english and dutch words \n",
    "eng_word = []\n",
    "nld_word = []\n",
    "for word in uncleaned_data_list:\n",
    "    eng_word.append(word.split('\\t')[:-1][0])\n",
    "    nld_word.append(word.split('\\t')[:-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2668d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating pandas df with english words and their dutch equivalents\n",
    "data = pd.DataFrame(columns=['English','Dutch'])\n",
    "data['English'] = eng_word\n",
    "data['Dutch'] = nld_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2c54576",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee046ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Dutch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Lopen!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vooruit.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hoi.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hé!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hai!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English     Dutch\n",
       "0     Go.    Lopen!\n",
       "1     Go.  Vooruit.\n",
       "2     Hi.      Hoi.\n",
       "3     Hi.       Hé!\n",
       "4     Hi.      Hai!"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6fc55a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"If someone who doesn't know your background says that you sound like a native speaker, it means they probably noticed something about your speaking that made them realize you weren't a native speaker. In other words, you don't really sound like a native speaker.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_word[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52423ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5eaeb871",
   "metadata": {},
   "outputs": [],
   "source": [
    "#80% of data used for training, and 20% for testing the models\n",
    "#Random state ensures that the splits that are generated are reproducible\n",
    "train_ex, test_ex = train_test_split(data,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87ff19c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_train = train_ex['English'].values\n",
    "nld_train = train_ex['Dutch'].values\n",
    "eng_test = test_ex['English'].values\n",
    "nld_test = test_ex['Dutch'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68d2dc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-30 04:22:55.298627: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-07-30 04:22:55.298651: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "#we get the slices of the arrays as objects by using tf.data.Dataset.from_tensor_slices() \n",
    "train_ex = tf.data.Dataset.from_tensor_slices((eng_train,nld_train))\n",
    "test_ex = tf.data.Dataset.from_tensor_slices((eng_test,nld_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4213d4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They walked.\n",
      "Ze hebben gelopen.\n"
     ]
    }
   ],
   "source": [
    "#tf.compat.as_text() converts any string-like python input types to unicode.\n",
    "for en, nld in train_ex.take(1):\n",
    "  print(tf.compat.as_text(en.numpy()))\n",
    "  print(tf.compat.as_text(nld.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0530af",
   "metadata": {},
   "source": [
    "# Preprocessing and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "260e9c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_unicode(text):\n",
    "    #strings are stored as unicode in Python 3.0 and above\n",
    "    if isinstance(text,str):\n",
    "        return text\n",
    "    #conversion of bytes to unicode\n",
    "    elif isinstance(text,bytes):\n",
    "        return text.decode('utf-8','ignore')\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported string type: %s\" % (type(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "31b2402c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load vocab file into a dictionary\n",
    "def load_dict(vocab_file):\n",
    "    vocab = collections.OrderedDict()\n",
    "    index = 0\n",
    "    with open(vocab_file, \"r\") as reader:\n",
    "        while True:\n",
    "            token = to_unicode(reader.readline())\n",
    "            if not token:\n",
    "                break\n",
    "            token = token.strip()\n",
    "            vocab[token] = index\n",
    "            index += 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3369206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes whitespace from text and returns tokens\n",
    "def remove_whitespace(text):\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    token = text.split()\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d765b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_vocab(vocab,items):\n",
    "    output = []\n",
    "    for item in items:\n",
    "        output.append(vocab[item])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "30799d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for end-to-end tokenization\n",
    "class FullTokenizer(object):\n",
    "    def __init__(self, vocab_file, do_lower_case=True):\n",
    "        self.vocab = load_dict(vocab_file)\n",
    "        # map IDs to tokens\n",
    "        self.invert_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        # basic tokenizer to break text into tokens based on whitespace\n",
    "        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
    "        # Wordpiece tokenizer to generate sub-tokens out of the tokens from BasicTokenizer\n",
    "        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n",
    "    \n",
    "    # uses Wordpiece to tokenize tokens generated by the BasicTokenizer\n",
    "    def tokenize(self, text):\n",
    "        split_tokens = []\n",
    "        for token in self.basic_tokenizer.tokenize(text):\n",
    "            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
    "                split_tokens.append(sub_token)\n",
    "        return split_tokens\n",
    "    \n",
    "    # after being provided the tokens, outputs the IDs of the tokens\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return convert_vocab(self.vocab, tokens)\n",
    "    \n",
    "    # after being provided the IDs, outputs the tokens mapped to the IDs\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        return convert_vocab(self.invert_vocab, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3828743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizes based on whitespace and punctuation, also lower-cases the tokens\n",
    "class BasicTokenizer(object):\n",
    "    \n",
    "    # whether or not to lower-case the tokens\n",
    "    def __init__(self, do_lower_case=True):\n",
    "        self.do_lower_case = do_lower_case\n",
    "\n",
    "    # tokenizes the text\n",
    "    def tokenize(self, text):\n",
    "        text = to_unicode(text)\n",
    "        text = self.text_clean(text)\n",
    "        text = self._tokenize_chinese_chars(text)\n",
    "\n",
    "        orig_tokens = remove_whitespace(text)\n",
    "        split_tokens = []\n",
    "        for token in orig_tokens:\n",
    "            if self.do_lower_case:\n",
    "                token = token.lower()\n",
    "                token = self._run_strip_accents(token)\n",
    "            split_tokens.extend(self._run_split_on_punc(token))\n",
    "\n",
    "        output_tokens = remove_whitespace(\" \".join(split_tokens))\n",
    "        return output_tokens\n",
    "\n",
    "    def _run_strip_accents(self, text):\n",
    "        \"\"\"Strips accents from a piece of text.\"\"\"\n",
    "        text = unicodedata.normalize(\"NFD\", text)\n",
    "        output = []\n",
    "        for char in text:\n",
    "            cat = unicodedata.category(char)\n",
    "            if cat == \"Mn\":\n",
    "                continue\n",
    "            output.append(char)\n",
    "        return \"\".join(output)\n",
    "\n",
    "    def _run_split_on_punc(self, text):\n",
    "        \"\"\"Splits punctuation on a piece of text.\"\"\"\n",
    "        chars = list(text)\n",
    "        i = 0\n",
    "        start_new_word = True\n",
    "        output = []\n",
    "        while i < len(chars):\n",
    "            char = chars[i]\n",
    "            if _is_punctuation(char):\n",
    "                output.append([char])\n",
    "                start_new_word = True\n",
    "            else:\n",
    "                if start_new_word:\n",
    "                    output.append([])\n",
    "                start_new_word = False\n",
    "                output[-1].append(char)\n",
    "            i += 1\n",
    "\n",
    "        return [\"\".join(x) for x in output]\n",
    "\n",
    "    # adds whitespace around CJK characters\n",
    "    def _tokenize_chinese_chars(self, text):\n",
    "        output = []\n",
    "        for char in text:\n",
    "            # returns unicode for the codepoint\n",
    "            cp = ord(char)\n",
    "            if self._is_chinese_char(cp):\n",
    "                output.append(\" \")\n",
    "                output.append(char)\n",
    "                output.append(\" \")\n",
    "            else:\n",
    "                output.append(char)\n",
    "        return \"\".join(output)\n",
    "    \n",
    "    # checks if codepoint is a CJK character\n",
    "    def _is_chinese_char(self, cp):\n",
    "        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n",
    "                (cp >= 0x3400 and cp <= 0x4DBF) or  #\n",
    "                (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n",
    "                (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n",
    "                (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n",
    "                (cp >= 0x2B820 and cp <= 0x2CEAF) or\n",
    "                (cp >= 0xF900 and cp <= 0xFAFF) or  #\n",
    "                (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "    \n",
    "    # removes invalid characters and cleans up whitespaces\n",
    "    def text_clean(self, text):\n",
    "        output = []\n",
    "        for char in text:\n",
    "            # get unicode of character\n",
    "            cp = ord(char)\n",
    "            # check if character is NULL, replacement character, or control character\n",
    "            if cp == 0 or cp == 0xfffd or _is_control(char):\n",
    "                continue\n",
    "            # check if character is a whitespace character\n",
    "            if _is_whitespace(char):\n",
    "                output.append(\" \")\n",
    "            else:\n",
    "                output.append(char)\n",
    "        return \"\".join(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "581357b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization based on Wordpiece\n",
    "class WordpieceTokenizer(object):\n",
    "\n",
    "    def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=200):\n",
    "        self.vocab = vocab\n",
    "        # unk_token represents an unknown word that is not present in the vocabulary\n",
    "        self.unk_token = unk_token\n",
    "        self.max_input_chars_per_word = max_input_chars_per_word\n",
    "    \n",
    "    # tokenizes text into word pieces\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        A very similar algorithm to BPE.\n",
    "        BPE is a compression algorithm which replaces consecutive bytes of data with a byte that does not occur\n",
    "        in the data. It does so by mapping individual characters of text to their frequency, inculding an EOW token.\n",
    "        In each further iteration, the most frequent pairing or characters or group of characters from the table\n",
    "        is merged together until token limit or iteration limit is reached.\n",
    "        \n",
    "        In WordPiece tokenization, the only difference is how the pairing to be merged is selected. At each \n",
    "        iterative step, WordPiece chooses a symbol pair which will result in the largest increase in likelihood \n",
    "        upon merging. P(AB)/[P(A)*P(B)]\n",
    "        The time complexity is O(K²) where K is the number of current word units in the table.\n",
    "        While we use probability, the algorithm is still greedy. For a probabilistic approach, the unigram tokenizer\n",
    "        is used.\n",
    "        \n",
    "        For example:\n",
    "          input = \"unaffable\"\n",
    "          output = [\"un\", \"##aff\", \"##able\"]\n",
    "        Here '##' indicates that the token is a suffix, and should be used in that context. \n",
    "    \n",
    "        Args:\n",
    "          text: A single token or whitespace separated tokens. This should have\n",
    "            already been passed through `BasicTokenizer.\n",
    "    \n",
    "        Returns:\n",
    "          A list of wordpiece tokens.\n",
    "        \"\"\"\n",
    "\n",
    "        text = to_unicode(text)\n",
    "\n",
    "        output_tokens = []\n",
    "        for token in remove_whitespace(text):\n",
    "            # splits token into chars\n",
    "            chars = list(token)\n",
    "            if len(chars) > self.max_input_chars_per_word:\n",
    "                output_tokens.append(self.unk_token)\n",
    "                continue\n",
    "\n",
    "            is_bad = False\n",
    "            start = 0\n",
    "            sub_tokens = []\n",
    "            while start < len(chars):\n",
    "                end = len(chars)\n",
    "                cur_substr = None\n",
    "                # finds longest substring from start of remaining word which is in vocab\n",
    "                while start < end:\n",
    "                    substr = \"\".join(chars[start:end])\n",
    "                    if start > 0:\n",
    "                        substr = \"##\" + substr\n",
    "                    if substr in self.vocab:\n",
    "                        cur_substr = substr\n",
    "                        break\n",
    "                    end -= 1\n",
    "                # if no such substring, the token is not in vocab\n",
    "                if cur_substr is None:\n",
    "                    is_bad = True\n",
    "                    break\n",
    "                sub_tokens.append(cur_substr)\n",
    "                # find next substring\n",
    "                start = end\n",
    "\n",
    "            if is_bad:\n",
    "                output_tokens.append(self.unk_token)\n",
    "            else:\n",
    "                output_tokens.extend(sub_tokens)\n",
    "        return output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b5e8286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks if char is whitespace \n",
    "def _is_whitespace(char):\n",
    "    # \\t, \\n, and \\r are technically controll characters but we treat them\n",
    "    # as whitespace since they are generally considered as such.\n",
    "    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        return True\n",
    "    # returns category of char\n",
    "    cat = unicodedata.category(char)\n",
    "    # char of category space separator\n",
    "    if cat == \"Zs\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _is_control(char):\n",
    "    # These are technically control characters but we count them as whitespace\n",
    "    # characters.\n",
    "    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        return False\n",
    "    # returns category of char\n",
    "    cat = unicodedata.category(char)\n",
    "    # char of category control, format, private use, or surrogate\n",
    "    if cat.startswith(\"C\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# checks if char is a punctuation\n",
    "def _is_punctuation(char):\n",
    "    cp = ord(char)\n",
    "    # We treat all non-letter/number ASCII as punctuation.\n",
    "    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
    "    # Punctuation class but we treat them as punctuation anyways, for\n",
    "    # consistency.\n",
    "    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
    "            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
    "        return True\n",
    "    # returns category of char\n",
    "    cat = unicodedata.category(char)\n",
    "    # char of category punctuation\n",
    "    if cat.startswith(\"P\"):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c7ec5b",
   "metadata": {},
   "source": [
    "### Create a custom subwords tokenizer from the training dataset for the decoder.\n",
    "The encoder uses BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2e806f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17285 ----> Mooie \n",
      "14012 ----> kale \n",
      "580 ----> man\n",
      "18097 ----> .\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "vocab_file = 'vocab_nld'\n",
    "# The vocabulary is \"trained\" on a corpus and all wordpieces are stored in a vocabulary file\n",
    "if os.path.isfile(vocab_file + '.subwords'):\n",
    "    # Invertible TextEncoder using word pieces with a byte-level fallback\n",
    "    tokenizer_nld = tfds.deprecated.text.SubwordTextEncoder.load_from_file(vocab_file)\n",
    "else: \n",
    "    # if vocab file not stored, build it\n",
    "    tokenizer_nld = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "      (nld_train), target_vocab_size=2 ** 15)\n",
    "    tokenizer_nld.save_to_file('vocab_nld')\n",
    "\n",
    "sample_string = 'Mooie kale man.'\n",
    "tokenized_string = tokenizer_nld.encode(sample_string)\n",
    "for ts in tokenized_string:\n",
    "  print ('{} ----> {}'.format(ts, tokenizer_nld.decode([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2e99602b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2498, 2064, 2022, 2589, 1012, 102]\n",
      "['[CLS]', 'nothing', 'can', 'be', 'done', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# english tokenizer using the custom FullTokenizer\n",
    "tokenizer_en = FullTokenizer(\n",
    "    vocab_file= 'vocab_en.txt',\n",
    "    do_lower_case=True)\n",
    "\n",
    "test_tokens = tokenizer_en.tokenize(eng_train[-1])\n",
    "# [CLS] token is used to indicate the task we want BERT to perform is next-sentence prediction, and not mask\n",
    "# word prediction. We can think about the output of [CLS] as a probability. Used to oraganize tasks as [CLS]\n",
    "# and [MASK]. [SEP] is also used for next-sentence predication tasks. It helps the model distinguish one \n",
    "# sentence from the next.\n",
    "test_ids = tokenizer_en.convert_tokens_to_ids(['[CLS]'] + test_tokens + ['[SEP]'])\n",
    "print(test_ids)\n",
    "print(tokenizer_en.convert_ids_to_tokens(test_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "44dc772e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 50\n",
    "\n",
    "def encode(en, nld, seq_length=MAX_SEQ_LENGTH):\n",
    "    # converts Python input text to unicode\n",
    "    tokens_en = tokenizer_en.tokenize(tf.compat.as_text(en.numpy()))\n",
    "    lang1 = tokenizer_en.convert_tokens_to_ids(['[CLS]'] + tokens_en + ['[SEP]'])\n",
    "    if len(lang1)<seq_length:\n",
    "        # makes tokens equal to length of seq_length\n",
    "        lang1 = lang1 + list(np.zeros(seq_length - len(lang1), 'int32'))\n",
    "    # ???\n",
    "    lang2 = [tokenizer_nld.vocab_size] + tokenizer_nld.encode(tf.compat.as_text(nld.numpy())) + [tokenizer_nld.vocab_size + 1]\n",
    "    if len(lang2)<seq_length:\n",
    "        lang2 = lang2 + list(np.zeros(seq_length - len(lang2), 'int32'))\n",
    "\n",
    "    return lang1, lang2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "059a93e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ???\n",
    "def tf_encode(en, nld):\n",
    "    # tf.py_function makes it possible to express control flow using Python constructs (if, while, for, etc.), \n",
    "    # instead of TensorFlow control flow constructs (tf.cond, tf.while_loop). \n",
    "    result_en, result_nld = tf.py_function(encode, [en, nld], [tf.int32, tf.int32])\n",
    "    # ???\n",
    "    result_en.set_shape([None])\n",
    "    result_nld.set_shape([None])\n",
    "\n",
    "    return result_en, result_nld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "284c7593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ???\n",
    "def filter_max_length(x, y, max_length=MAX_SEQ_LENGTH):\n",
    "    # element-wise AND of its arguments\n",
    "    return tf.logical_and(tf.size(x) <= max_length,\n",
    "                        tf.size(y) <= max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b39fc057",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 40000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "#applies tf_encode, filter_max_length functions to entire dataset\n",
    "train_dataset = train_ex.map(tf_encode)\n",
    "train_dataset = train_dataset.filter(filter_max_length)\n",
    "\n",
    "# cache the dataset to memory to get a speedup while reading from it.\n",
    "train_dataset = train_dataset.cache()\n",
    "# maintains a fixed-size buffer and chooses the next element uniformly at random from that buffer.\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(\n",
    "    BATCH_SIZE, padded_shapes=([-1], [-1]), drop_remainder=True)\n",
    "# creates a Dataset that prefetches elements from this dataset. This allows later elements to \n",
    "# be prepared while the current element is being processed. \n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "test_dataset = test_ex.map(\n",
    "    lambda en, nld: tf.py_function(encode, [en, nld], [tf.int32, tf.int32]))\n",
    "test_dataset = test_dataset.filter(filter_max_length)\n",
    "test_dataset = test_dataset.padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e491c613",
   "metadata": {},
   "source": [
    "# Positional Encoding\n",
    "The positional encoding vector is added to the embedding vector. Embeddings represent a token in a \n",
    "d-dimensional space where tokens with similar meaning will be closer to each other. But the embeddings do \n",
    "not encode the relative position of words in a sentence. So after adding the positional encoding, words will\n",
    "be closer to each other based on the similarity of their meaning and their position in the sentence, in the \n",
    "d-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c107e85b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniforge3-tensorflow] *",
   "language": "python",
   "name": "conda-env-miniforge3-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
